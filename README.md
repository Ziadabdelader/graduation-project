# graduation_project-Depi (Integrated Data Infrastructure(online Retail)
 
#  Overview
This project demonstrates the development of an efficient and scalable ETL (Extract, Transform, Load) data pipeline that processes data from multiple sources (CSV, XML, JSON) and loads it into a centralized data warehouse.
 We leveraged various tools and technologies such as Python, SQL, SSIS, PostgreSQL, MySQL, Hadoop, Apache Spark, and Power BI to perform advanced data analysis and visualizations.

The project follows a distributed system approach for handling large-scale data processing, with a focus on real-time analytics and interactive data visualizations for decision-making.

# Key Features
ETL Process: Built using Python and SQL for extracting, transforming, and loading data into PostgreSQL, MySQL, and a data warehouse.
Data Warehousing: Designed and implemented a star schema using SSIS for optimal data storage and retrieval, supporting OLAP for analytical processing.
Big Data Tools: Utilized Hadoop for distributed storage and Apache Spark for fast in-memory computation, improving performance for large-scale data analysis.
Data Visualization: Created interactive visualizations using Python (Matplotlib, Seaborn) and Power BI, providing insights into key metrics and trends.
Real-time Dashboards: Integrated PySpark with Tableau for dynamic, real-time data visualization, helping with sales data analysis.

# Technologies Used
Python: Data processing, ETL, and analysis.
PostgreSQL & MySQL: Databases for initial data storage and SQL-based querying.
SSIS: ETL tool for data warehousing.
Hadoop & Apache Spark: Big data frameworks for distributed storage and real-time data processing.
Power BI & Python: Tools for creating custom visualizations and interactive dashboards.

# Acknowledgements
We extend our gratitude to DEPI and Engineer Hazem for their support and guidance throughout the project.


  
